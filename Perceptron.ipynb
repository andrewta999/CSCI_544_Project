{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "45730f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from json import load\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea481be7",
   "metadata": {},
   "source": [
    "## Read CSV and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a4386b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WINSTO~1\\AppData\\Local\\Temp/ipykernel_17992/3503525313.py:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  csvDF = readCSV(\"./USvideos.csv\")\n",
      "b'Skipping line 2401: expected 11 fields, saw 21\\nSkipping line 2800: expected 11 fields, saw 21\\nSkipping line 5297: expected 11 fields, saw 12\\nSkipping line 5299: expected 11 fields, saw 12\\nSkipping line 5300: expected 11 fields, saw 12\\nSkipping line 5301: expected 11 fields, saw 12\\n'\n",
      "C:\\Users\\WINSTO~1\\AppData\\Local\\Temp/ipykernel_17992/3503525313.py:22: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  csvDF = readCSV(\"./USvideos.csv\")\n",
      "b'Skipping line 2401: expected 11 fields, saw 21\\nSkipping line 2800: expected 11 fields, saw 21\\nSkipping line 5297: expected 11 fields, saw 12\\nSkipping line 5299: expected 11 fields, saw 12\\nSkipping line 5300: expected 11 fields, saw 12\\nSkipping line 5301: expected 11 fields, saw 12\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 'Film & Animation',\n",
       " 2: 'Autos & Vehicles',\n",
       " 10: 'Music',\n",
       " 15: 'Pets & Animals',\n",
       " 17: 'Sports',\n",
       " 18: 'Short Movies',\n",
       " 19: 'Travel & Events',\n",
       " 20: 'Gaming',\n",
       " 21: 'Videoblogging',\n",
       " 22: 'People & Blogs',\n",
       " 23: 'Comedy',\n",
       " 24: 'Entertainment',\n",
       " 25: 'News & Politics',\n",
       " 26: 'Howto & Style',\n",
       " 27: 'Education',\n",
       " 28: 'Science & Technology',\n",
       " 29: 'Nonprofits & Activism',\n",
       " 30: 'Movies',\n",
       " 31: 'Anime/Animation',\n",
       " 32: 'Action/Adventure',\n",
       " 33: 'Classics',\n",
       " 34: 'Comedy',\n",
       " 35: 'Documentary',\n",
       " 36: 'Drama',\n",
       " 37: 'Family',\n",
       " 38: 'Foreign',\n",
       " 39: 'Horror',\n",
       " 40: 'Sci-Fi/Fantasy',\n",
       " 41: 'Thriller',\n",
       " 42: 'Shorts',\n",
       " 43: 'Shows',\n",
       " 44: 'Trailers'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readCSV(dir):\n",
    "    df = pd.read_csv(dir, error_bad_lines = False)\n",
    "    return df\n",
    "\n",
    "csvDF = readCSV(\"./USvideos.csv\")\n",
    "\n",
    "# takes in the YouTube categories JSON file and returns a dictionary\n",
    "# where each key is a category ID and each value is a category\n",
    "def readJSON(dir):\n",
    "    categoryDictionary = {}\n",
    "    \n",
    "    f = open(dir)\n",
    "    jsonDictionary = load(f)\n",
    "    # iterate through all the categories in the JSON dictionary\n",
    "    for category in jsonDictionary[\"items\"]:\n",
    "        currentID = int(category[\"id\"])\n",
    "        currentCategory = category[\"snippet\"][\"title\"]\n",
    "        categoryDictionary[currentID] = currentCategory\n",
    "    f.close()\n",
    "    return categoryDictionary\n",
    "\n",
    "csvDF = readCSV(\"./USvideos.csv\")\n",
    "jsonCategories = readJSON(\"./US_category_id.json\")\n",
    "jsonCategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ad748c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       24\n",
       "1       28\n",
       "2       22\n",
       "3       28\n",
       "4       23\n",
       "        ..\n",
       "7987    27\n",
       "7988    25\n",
       "7989    10\n",
       "7990    24\n",
       "7991    28\n",
       "Name: category_id, Length: 7992, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorizeDF(csvDF, jsonCategories):\n",
    "    csvDF[\"category\"] = csvDF[\"category_id\"].map(lambda x: jsonCategories[x])\n",
    "    csvDF[\"tagsCleaned\"] = csvDF[\"tags\"].map(lambda x: x.replace(\"|\", \" \"))\n",
    "    csvDF[\"tagsCleaned\"] = csvDF[\"tagsCleaned\"].map(lambda x: x.replace(\"[none]\", \"\"))\n",
    "    csvDF[\"titlechanneltags\"] = csvDF[\"title\"] + \" \" + csvDF[\"channel_title\"] + \" \" + csvDF[\"tagsCleaned\"]\n",
    "    filteredDF = csvDF.filter([\"titlechanneltags\"], axis = 1)\n",
    "    return filteredDF\n",
    "    \n",
    "filteredDF = categorizeDF(csvDF, jsonCategories)\n",
    "classDF = csvDF[\"category_id\"]\n",
    "classDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afda38c",
   "metadata": {},
   "source": [
    "## Split Testing and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a9f205f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in the reduced dataframe and the class dataframe and splits it for test and train split\n",
    "def splitDF(dfX, dfY):\n",
    "    return train_test_split(dfX, dfY, train_size = 0.8)\n",
    "\n",
    "trainX, testX, trainY, testY = splitDF(filteredDF, classDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b27f1",
   "metadata": {},
   "source": [
    "## Load Gensim's Pre-Trained Model: word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d90a3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f83d8",
   "metadata": {},
   "source": [
    "## Vectorize the review bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e78c6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function with takes as input an array of word tokens and the word vectors \n",
    "# dictionary, and outputs a word vector for that entire review\n",
    "def reviewVectorizer(review_body_arr, wordVectors):\n",
    "    total, wordCount = 0, 0\n",
    "    for word in review_body_arr:\n",
    "        try:\n",
    "            total += wordVectors[word]\n",
    "            wordCount += 1\n",
    "        except: continue\n",
    "    if wordCount == 0: return [0] * 300\n",
    "    featureValue = total / len(review_body_arr)\n",
    "    return featureValue\n",
    "\n",
    "# takes in as input the training or test data, the word vectors dictionary,\n",
    "# a parameter for whether or not to tokenize the data, and an optional filename\n",
    "# in case outputting to memory for preprocessing purposes and\n",
    "# returns a dataframe with the vectors corresponding to each review\n",
    "def word2VecCSV(dataFile, wordVector):\n",
    "    dataFile[\"titlechanneltags_tokens\"] = dataFile[\"titlechanneltags\"].apply(lambda x: x.split(\" \"))\n",
    "    dataFile[\"titlechanneltagsWordVector\"] = dataFile[\"titlechanneltags_tokens\"].apply(lambda x: reviewVectorizer(x, wordVector))\n",
    "    colDF = pd.DataFrame(dataFile[\"titlechanneltagsWordVector\"].to_list())\n",
    "    return colDF\n",
    "\n",
    "perceptronTrainX = word2VecCSV(trainX, wv)\n",
    "perceptronTestX  = word2VecCSV(testX, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81449e21",
   "metadata": {},
   "source": [
    "## Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "12d5ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes as input the review data, and performs a perceptron\n",
    "# model analysis and prediction on it, returning the results\n",
    "# dataframe\n",
    "def perceptron(trainX, trainY, testX, testY):\n",
    "    model = Perceptron(tol = 0.001, random_state = 0)\n",
    "    results = model.fit(trainX, trainY)\n",
    "    predY = results.predict(testX)\n",
    "    resultDF = pd.DataFrame(data = predY, columns = [\"Prediction\"])\n",
    "    testY = testY.reset_index(drop = True)\n",
    "    resultDF = pd.concat([resultDF, testY], axis = 1)\n",
    "    return resultDF\n",
    "\n",
    "perceptronResults = perceptron(perceptronTrainX, trainY, perceptronTestX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9586c3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Perceptron\n",
      "Accuracy: 0.7773608505315822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# takes in as input a results dataframe from a model, as well \n",
    "# as a string for the model name, and then outputs \n",
    "# metrics relating to that model's results\n",
    "def metricPrinter(df, modelString):\n",
    "    print(\"Model:\", modelString)\n",
    "    resultPredictions = df[\"Prediction\"]\n",
    "    resultActuals = df[\"category_id\"]\n",
    "    precision = precision_score(resultActuals, resultPredictions, average = None)\n",
    "    recall = recall_score(resultActuals, resultPredictions, average = None)\n",
    "    f1 = f1_score(resultActuals, resultPredictions, average = None)\n",
    "    accuracy = accuracy_score(resultActuals, resultPredictions)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "metricPrinter(perceptronResults, \"Perceptron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106f5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
